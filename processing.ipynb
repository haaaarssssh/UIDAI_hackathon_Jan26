{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd4f2326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory 'data/level1_processing' is ready.\n",
      "Found 4 files for 'biometric'. Merging now...\n",
      "Successfully merged and saved to 'data/level1_processing\\processed_biometric_data.csv'\n",
      "\n",
      "Found 5 files for 'demographic'. Merging now...\n",
      "Successfully merged and saved to 'data/level1_processing\\processed_demographic_data.csv'\n",
      "\n",
      "Found 3 files for 'enrolment'. Merging now...\n",
      "Successfully merged and saved to 'data/level1_processing\\processed_enrolment_data.csv'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "def process_and_merge_data(raw_folder='raw_files', output_folder='data/level1_processing'):\n",
    "    \"\"\"\n",
    "    Merges CSV files from a specified raw data folder based on keywords\n",
    "    and saves them into a designated output folder.\n",
    "\n",
    "    This function assumes it is run from the root directory where the \n",
    "    'raw_files' folder is located.\n",
    "\n",
    "    Args:\n",
    "        raw_folder (str): The name of the folder containing the raw CSV files.\n",
    "        output_folder (str): The name of the folder where the merged CSV files will be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1. Create the output directory if it doesn't exist ---\n",
    "    # The `exist_ok=True` argument prevents an error if the directory already exists.\n",
    "    try:\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        print(f\"Output directory '{output_folder}' is ready.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error creating directory {output_folder}: {e}\")\n",
    "        return # Exit the function if directory creation fails\n",
    "\n",
    "    # --- 2. Define the keywords to search for in filenames ---\n",
    "    keywords = ['biometric', 'demographic', 'enrolment']\n",
    "\n",
    "    # --- 3. Loop through each keyword to process files ---\n",
    "    for keyword in keywords:\n",
    "        # Construct a search pattern to find all files for the current keyword\n",
    "        search_pattern = os.path.join(raw_folder, f'*api_data_aadhar_{keyword}*.csv')\n",
    "        \n",
    "        # Use glob to find all file paths that match the pattern\n",
    "        files_to_merge = glob.glob(search_pattern)\n",
    "\n",
    "        # Check if any files were found\n",
    "        if not files_to_merge:\n",
    "            print(f\"Warning: No files found for keyword '{keyword}' in '{raw_folder}'.\")\n",
    "            continue # Skip to the next keyword\n",
    "\n",
    "        print(f\"Found {len(files_to_merge)} files for '{keyword}'. Merging now...\")\n",
    "\n",
    "        # --- 4. Read and combine the found CSV files ---\n",
    "        list_of_dataframes = [pd.read_csv(file) for file in files_to_merge]\n",
    "        \n",
    "        if list_of_dataframes:\n",
    "            # Concatenate all dataframes into a single one\n",
    "            merged_df = pd.concat(list_of_dataframes, ignore_index=True)\n",
    "\n",
    "            # --- 5. Save the merged dataframe to the output folder ---\n",
    "            output_filename = os.path.join(output_folder, f'processed_{keyword}_data.csv')\n",
    "            merged_df.to_csv(output_filename, index=False)\n",
    "            print(f\"Successfully merged and saved to '{output_filename}'\\n\")\n",
    "\n",
    "# --- Example of how to run the function ---\n",
    "# To run this, make sure you have:\n",
    "# 1. This script in your project's root folder.\n",
    "# 2. A folder named 'raw_files' in the same root folder, containing your CSVs.\n",
    "\n",
    "process_and_merge_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e3c667e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the process of finding unique states...\n",
      "Successfully read processed_demographic_data.csv.\n",
      "  Found 65 unique state entries in 'state' column.\n",
      "Successfully read processed_enrolment_data.csv.\n",
      "  Found 55 unique state entries in 'state' column.\n",
      "Successfully read processed_biometric_data.csv.\n",
      "  Found 57 unique state entries in 'state' column.\n",
      "\n",
      "Process complete. Found 68 unique states.\n",
      "The list of unique states has been saved to 'unique_states.txt'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def find_unique_states():\n",
    "    \"\"\"\n",
    "    Reads multiple processed CSV files, extracts unique state names,\n",
    "    and saves them to a text file.\n",
    "    \"\"\"\n",
    "    # Define the directory and the list of files to process\n",
    "    data_dir = os.path.join('data', 'level1_processing')\n",
    "    files_to_process = [\n",
    "        'processed_demographic_data.csv',\n",
    "        'processed_enrolment_data.csv',\n",
    "        'processed_biometric_data.csv'\n",
    "    ]\n",
    "\n",
    "    # A set is used to automatically store unique state names\n",
    "    unique_states = set()\n",
    "\n",
    "    # List of possible column names for the state field\n",
    "    # (to make the script more robust)\n",
    "    possible_state_columns = ['state', 'State', 'state_name', 'State_Name']\n",
    "\n",
    "    print(\"Starting the process of finding unique states...\")\n",
    "\n",
    "    # Loop through each file\n",
    "    for filename in files_to_process:\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "\n",
    "        try:\n",
    "            # Read the CSV file into a pandas DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"Successfully read {filename}.\")\n",
    "\n",
    "            # Identify the correct state column in the DataFrame\n",
    "            state_column = None\n",
    "            for col in possible_state_columns:\n",
    "                if col in df.columns:\n",
    "                    state_column = col\n",
    "                    break\n",
    "            \n",
    "            if state_column:\n",
    "                # Drop any missing values and get the unique states\n",
    "                found_states = df[state_column].dropna().unique()\n",
    "                # Add the found states to our set\n",
    "                unique_states.update(found_states)\n",
    "                print(f\"  Found {len(found_states)} unique state entries in '{state_column}' column.\")\n",
    "            else:\n",
    "                print(f\"  Warning: No state column found in {filename}.\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: The file {filename} was not found in {data_dir}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {filename}: {e}\")\n",
    "\n",
    "    # Check if any states were found\n",
    "    if not unique_states:\n",
    "        print(\"No state information was found in any of the files.\")\n",
    "        return\n",
    "\n",
    "    # Sort the states alphabetically\n",
    "    sorted_states = sorted(list(unique_states))\n",
    "\n",
    "    # Define the output file path\n",
    "    output_filename = 'unique_states.txt'\n",
    "\n",
    "    try:\n",
    "        # Write the sorted list of states to the output file.\n",
    "        with open(output_filename, 'w') as f:\n",
    "            for state in sorted_states:\n",
    "                f.write(f\"{state}\\n\")\n",
    "        \n",
    "        print(f\"\\nProcess complete. Found {len(sorted_states)} unique states.\")\n",
    "        print(f\"The list of unique states has been saved to '{output_filename}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing to the output file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # To run this script, save it as a Python file (e.g., `process_states.py`)\n",
    "    # in the 'UIDAI HACKATHON' root directory and run it from your terminal.\n",
    "    find_unique_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e572253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'data\\level2_processing' is ready.\n",
      "\n",
      "Starting level-1 data cleaning process...\n",
      "\n",
      "Processing 'processed_biometric_data.csv'...\n",
      "  Found state data in column: 'state'.\n",
      "  Removed 0 non-state rows.\n",
      "  Removed 0 rows with unmapped state names.\n",
      "  Successfully cleaned and saved to 'data\\level2_processing\\processed_biometric_data.csv'. Final row count: 1861108.\n",
      "\n",
      "Processing 'processed_demographic_data.csv'...\n",
      "  Found state data in column: 'state'.\n",
      "  Removed 13 non-state rows.\n",
      "  Removed 0 rows with unmapped state names.\n",
      "  Successfully cleaned and saved to 'data\\level2_processing\\processed_demographic_data.csv'. Final row count: 2071687.\n",
      "\n",
      "Processing 'processed_enrolment_data.csv'...\n",
      "  Found state data in column: 'state'.\n",
      "  Removed 22 non-state rows.\n",
      "  Removed 0 rows with unmapped state names.\n",
      "  Successfully cleaned and saved to 'data\\level2_processing\\processed_enrolment_data.csv'. Final row count: 1006007.\n",
      "\n",
      "--- Data cleaning process complete. ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def clean_and_process_files():\n",
    "    \"\"\"\n",
    "    Cleans the state column in level-1 CSV files by mapping incorrect names\n",
    "    to official names, removing non-state rows, and saves the cleaned files\n",
    "    to a level-2 directory.\n",
    "    \"\"\"\n",
    "    # Define source and destination directories\n",
    "    level1_dir = os.path.join('data', 'level1_processing')\n",
    "    level2_dir = os.path.join('data', 'level2_processing')\n",
    "\n",
    "    # Create the level-2 directory if it doesn't exist\n",
    "    try:\n",
    "        os.makedirs(level2_dir, exist_ok=True)\n",
    "        print(f\"Directory '{level2_dir}' is ready.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error creating directory {level2_dir}: {e}\")\n",
    "        return\n",
    "\n",
    "    # List of files to process from the image\n",
    "    files_to_process = [\n",
    "        'processed_biometric_data.csv',\n",
    "        'processed_demographic_data.csv',\n",
    "        'processed_enrolment_data.csv'\n",
    "    ]\n",
    "\n",
    "    # --- Data Cleaning Definitions ---\n",
    "\n",
    "    # 1. Define the mapping for all variations to the official state name.\n",
    "    #    Keys are lowercase for case-insensitive matching.\n",
    "    state_mapping = {\n",
    "        'andaman & nicobar islands': 'Andaman and Nicobar Islands',\n",
    "        'andaman and nicobar islands': 'Andaman and Nicobar Islands',\n",
    "        'andhra pradesh': 'Andhra Pradesh',\n",
    "        'arunachal pradesh': 'Arunachal Pradesh',\n",
    "        'assam': 'Assam',\n",
    "        'bihar': 'Bihar',\n",
    "        'chandigarh': 'Chandigarh',\n",
    "        'chhatisgarh': 'Chhattisgarh',\n",
    "        'chhattisgarh': 'Chhattisgarh',\n",
    "        'dadra & nagar haveli': 'Dadra and Nagar Haveli and Daman and Diu',\n",
    "        'dadra and nagar haveli': 'Dadra and Nagar Haveli and Daman and Diu',\n",
    "        'daman & diu': 'Dadra and Nagar Haveli and Daman and Diu',\n",
    "        'daman and diu': 'Dadra and Nagar Haveli and Daman and Diu',\n",
    "        'dadra and nagar haveli and daman and diu': 'Dadra and Nagar Haveli and Daman and Diu',\n",
    "        'the dadra and nagar haveli and daman and diu': 'Dadra and Nagar Haveli and Daman and Diu',\n",
    "        'delhi': 'Delhi',\n",
    "        'goa': 'Goa',\n",
    "        'gujarat': 'Gujarat',\n",
    "        'haryana': 'Haryana',\n",
    "        'himachal pradesh': 'Himachal Pradesh',\n",
    "        'jammu & kashmir': 'Jammu and Kashmir',\n",
    "        'jammu and kashmir': 'Jammu and Kashmir',\n",
    "        'jharkhand': 'Jharkhand',\n",
    "        'karnataka': 'Karnataka',\n",
    "        'kerala': 'Kerala',\n",
    "        'ladakh': 'Ladakh',\n",
    "        'lakshadweep': 'Lakshadweep',\n",
    "        'madhya pradesh': 'Madhya Pradesh',\n",
    "        'maharashtra': 'Maharashtra',\n",
    "        'manipur': 'Manipur',\n",
    "        'meghalaya': 'Meghalaya',\n",
    "        'mizoram': 'Mizoram',\n",
    "        'nagaland': 'Nagaland',\n",
    "        'odisha': 'Odisha',\n",
    "        'orissa': 'Odisha',\n",
    "        'pondicherry': 'Puducherry',\n",
    "        'puducherry': 'Puducherry',\n",
    "        'punjab': 'Punjab',\n",
    "        'rajasthan': 'Rajasthan',\n",
    "        'sikkim': 'Sikkim',\n",
    "        'tamil nadu': 'Tamil Nadu',\n",
    "        'tamilnadu': 'Tamil Nadu',\n",
    "        'telangana': 'Telangana',\n",
    "        'tripura': 'Tripura',\n",
    "        'uttar pradesh': 'Uttar Pradesh',\n",
    "        'uttarakhand': 'Uttarakhand',\n",
    "        'uttaranchal': 'Uttarakhand',\n",
    "        'west bengal': 'West Bengal',\n",
    "        'west bangal': 'West Bengal',\n",
    "        'westbengal': 'West Bengal',\n",
    "        'west bengli': 'West Bengal',\n",
    "        'west  bengal': 'West Bengal' # Handles extra spaces\n",
    "    }\n",
    "\n",
    "    # 2. Define non-state entries to be removed.\n",
    "    #    Using a set for efficient lookup. Stored in lowercase.\n",
    "    non_states_to_remove = {\n",
    "        '100000',\n",
    "        'balanagar',\n",
    "        'darbhanga',\n",
    "        'jaipur',\n",
    "        'madanapalle',\n",
    "        'nagpur',\n",
    "        'puttenahalli',\n",
    "        'raja annamalai puram'\n",
    "    }\n",
    "\n",
    "    # --- Processing Loop ---\n",
    "    print(\"\\nStarting level-1 data cleaning process...\")\n",
    "\n",
    "    for filename in files_to_process:\n",
    "        input_path = os.path.join(level1_dir, filename)\n",
    "        output_path = os.path.join(level2_dir, filename)\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(input_path)\n",
    "            print(f\"\\nProcessing '{filename}'...\")\n",
    "\n",
    "            # Find the state column (robustly checks for common names)\n",
    "            state_column = None\n",
    "            for col in ['state', 'State', 'state_name', 'State_Name']:\n",
    "                if col in df.columns:\n",
    "                    state_column = col\n",
    "                    break\n",
    "            \n",
    "            if not state_column:\n",
    "                print(f\"  Warning: No state column found in '{filename}'. Skipping cleaning for this file.\")\n",
    "                # Optionally, you can copy the file as-is to level2 directory\n",
    "                # shutil.copy(input_path, output_path)\n",
    "                continue\n",
    "\n",
    "            print(f\"  Found state data in column: '{state_column}'.\")\n",
    "            original_rows = len(df)\n",
    "\n",
    "            # Create a standardized, lowercase version of the state column for matching\n",
    "            # .astype(str) handles potential non-string data. .strip() removes whitespace.\n",
    "            df['state_lower'] = df[state_column].astype(str).str.lower().str.strip()\n",
    "            \n",
    "            # First, remove rows that are not states\n",
    "            df = df[~df['state_lower'].isin(non_states_to_remove)]\n",
    "            rows_after_removal = len(df)\n",
    "            print(f\"  Removed {original_rows - rows_after_removal} non-state rows.\")\n",
    "\n",
    "            # Apply the mapping to correct state names\n",
    "            df[state_column] = df['state_lower'].map(state_mapping)\n",
    "\n",
    "            # Drop the temporary lowercase column\n",
    "            df = df.drop(columns=['state_lower'])\n",
    "            \n",
    "            # Remove any rows that didn't match a state in our mapping\n",
    "            df = df.dropna(subset=[state_column])\n",
    "            rows_after_mapping = len(df)\n",
    "            print(f\"  Removed {rows_after_removal - rows_after_mapping} rows with unmapped state names.\")\n",
    "            \n",
    "            # Save the cleaned DataFrame to the level-2 directory\n",
    "            df.to_csv(output_path, index=False)\n",
    "            print(f\"  Successfully cleaned and saved to '{output_path}'. Final row count: {len(df)}.\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: The file '{filename}' was not found in '{level1_dir}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred while processing '{filename}': {e}\")\n",
    "    \n",
    "    print(\"\\n--- Data cleaning process complete. ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clean_and_process_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f18e48dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Checking for unique states in 'data\\level2_processing' ---\n",
      "Reading 'processed_biometric_data.csv'...\n",
      "  Found 36 unique state entries in this file.\n",
      "Reading 'processed_demographic_data.csv'...\n",
      "  Found 36 unique state entries in this file.\n",
      "Reading 'processed_enrolment_data.csv'...\n",
      "  Found 36 unique state entries in this file.\n",
      "\n",
      "--- Verification Complete ---\n",
      "Total number of unique states and UTs found: 36\n",
      "---------------------------------\n",
      "Andaman and Nicobar Islands\n",
      "Andhra Pradesh\n",
      "Arunachal Pradesh\n",
      "Assam\n",
      "Bihar\n",
      "Chandigarh\n",
      "Chhattisgarh\n",
      "Dadra and Nagar Haveli and Daman and Diu\n",
      "Delhi\n",
      "Goa\n",
      "Gujarat\n",
      "Haryana\n",
      "Himachal Pradesh\n",
      "Jammu and Kashmir\n",
      "Jharkhand\n",
      "Karnataka\n",
      "Kerala\n",
      "Ladakh\n",
      "Lakshadweep\n",
      "Madhya Pradesh\n",
      "Maharashtra\n",
      "Manipur\n",
      "Meghalaya\n",
      "Mizoram\n",
      "Nagaland\n",
      "Odisha\n",
      "Puducherry\n",
      "Punjab\n",
      "Rajasthan\n",
      "Sikkim\n",
      "Tamil Nadu\n",
      "Telangana\n",
      "Tripura\n",
      "Uttar Pradesh\n",
      "Uttarakhand\n",
      "West Bengal\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def check_unique_states_level2():\n",
    "    \"\"\"\n",
    "    Reads the cleaned level-2 CSV files, extracts all unique state names,\n",
    "    and prints the consolidated list to the console for verification.\n",
    "    \"\"\"\n",
    "    # Define the directory where the cleaned files are located\n",
    "    level2_dir = os.path.join('data', 'level2_processing')\n",
    "\n",
    "    # Check if the directory exists before proceeding\n",
    "    if not os.path.isdir(level2_dir):\n",
    "        print(f\"Error: The directory '{level2_dir}' was not found.\")\n",
    "        print(\"Please run the cleaning script first to generate the level-2 files.\")\n",
    "        return\n",
    "\n",
    "    # List of files to process\n",
    "    files_to_process = [\n",
    "        'processed_biometric_data.csv',\n",
    "        'processed_demographic_data.csv',\n",
    "        'processed_enrolment_data.csv'\n",
    "    ]\n",
    "\n",
    "    # Use a set to automatically store unique state names\n",
    "    unique_states = set()\n",
    "\n",
    "    # List of possible column names for the state field\n",
    "    possible_state_columns = ['state', 'State', 'state_name', 'State_Name']\n",
    "\n",
    "    print(f\"--- Checking for unique states in '{level2_dir}' ---\")\n",
    "\n",
    "    # Loop through each file\n",
    "    for filename in files_to_process:\n",
    "        file_path = os.path.join(level2_dir, filename)\n",
    "\n",
    "        try:\n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"Reading '{filename}'...\")\n",
    "\n",
    "            # Identify the state column in the DataFrame\n",
    "            state_column = None\n",
    "            for col in possible_state_columns:\n",
    "                if col in df.columns:\n",
    "                    state_column = col\n",
    "                    break\n",
    "            \n",
    "            if state_column:\n",
    "                # Get the unique, non-null states from the column\n",
    "                found_states = df[state_column].dropna().unique()\n",
    "                # Add the found states to our master set\n",
    "                unique_states.update(found_states)\n",
    "                print(f\"  Found {len(found_states)} unique state entries in this file.\")\n",
    "            else:\n",
    "                print(f\"  Warning: No state column found in {filename}.\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"  Error: The file '{filename}' was not found. Skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  An error occurred while processing {filename}: {e}\")\n",
    "\n",
    "    # --- Display the final results ---\n",
    "    if not unique_states:\n",
    "        print(\"\\nNo state information could be found in the level-2 files.\")\n",
    "        return\n",
    "\n",
    "    # Convert the set to a sorted list for clean display\n",
    "    sorted_states = sorted(list(unique_states))\n",
    "\n",
    "    print(\"\\n--- Verification Complete ---\")\n",
    "    print(f\"Total number of unique states and UTs found: {len(sorted_states)}\")\n",
    "    print(\"---------------------------------\")\n",
    "    \n",
    "    # Print each state name\n",
    "    for state in sorted_states:\n",
    "        print(state)\n",
    "    \n",
    "    print(\"---------------------------------\")\n",
    "\n",
    "\n",
    "check_unique_states_level2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d05defe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output will be saved in 'state_wise_districts' directory.\n",
      "\n",
      "Reading and combining level-2 files...\n",
      "  Successfully loaded 'processed_biometric_data.csv'.\n",
      "  Successfully loaded 'processed_demographic_data.csv'.\n",
      "  Successfully loaded 'processed_enrolment_data.csv'.\n",
      "Combined data contains 4938802 total rows.\n",
      "Using 'state' for states and 'district' for districts.\n",
      "Removed 0 rows with missing state or district info.\n",
      "\n",
      "Processing data for each state...\n",
      "  -> Saved 5 unique districts for 'Andaman and Nicobar Islands' to 'Andaman_and_Nicobar_Islands_districts.txt'\n",
      "  -> Saved 49 unique districts for 'Andhra Pradesh' to 'Andhra_Pradesh_districts.txt'\n",
      "  -> Saved 25 unique districts for 'Arunachal Pradesh' to 'Arunachal_Pradesh_districts.txt'\n",
      "  -> Saved 38 unique districts for 'Assam' to 'Assam_districts.txt'\n",
      "  -> Saved 48 unique districts for 'Bihar' to 'Bihar_districts.txt'\n",
      "  -> Saved 3 unique districts for 'Chandigarh' to 'Chandigarh_districts.txt'\n",
      "  -> Saved 41 unique districts for 'Chhattisgarh' to 'Chhattisgarh_districts.txt'\n",
      "  -> Saved 5 unique districts for 'Dadra and Nagar Haveli and Daman and Diu' to 'Dadra_and_Nagar_Haveli_and_Daman_and_Diu_districts.txt'\n",
      "  -> Saved 14 unique districts for 'Delhi' to 'Delhi_districts.txt'\n",
      "  -> Saved 5 unique districts for 'Goa' to 'Goa_districts.txt'\n",
      "  -> Saved 40 unique districts for 'Gujarat' to 'Gujarat_districts.txt'\n",
      "  -> Saved 27 unique districts for 'Haryana' to 'Haryana_districts.txt'\n",
      "  -> Saved 14 unique districts for 'Himachal Pradesh' to 'Himachal_Pradesh_districts.txt'\n",
      "  -> Saved 31 unique districts for 'Jammu and Kashmir' to 'Jammu_and_Kashmir_districts.txt'\n",
      "  -> Saved 35 unique districts for 'Jharkhand' to 'Jharkhand_districts.txt'\n",
      "  -> Saved 56 unique districts for 'Karnataka' to 'Karnataka_districts.txt'\n",
      "  -> Saved 15 unique districts for 'Kerala' to 'Kerala_districts.txt'\n",
      "  -> Saved 2 unique districts for 'Ladakh' to 'Ladakh_districts.txt'\n",
      "  -> Saved 1 unique districts for 'Lakshadweep' to 'Lakshadweep_districts.txt'\n",
      "  -> Saved 61 unique districts for 'Madhya Pradesh' to 'Madhya_Pradesh_districts.txt'\n",
      "  -> Saved 54 unique districts for 'Maharashtra' to 'Maharashtra_districts.txt'\n",
      "  -> Saved 13 unique districts for 'Manipur' to 'Manipur_districts.txt'\n",
      "  -> Saved 14 unique districts for 'Meghalaya' to 'Meghalaya_districts.txt'\n",
      "  -> Saved 12 unique districts for 'Mizoram' to 'Mizoram_districts.txt'\n",
      "  -> Saved 17 unique districts for 'Nagaland' to 'Nagaland_districts.txt'\n",
      "  -> Saved 53 unique districts for 'Odisha' to 'Odisha_districts.txt'\n",
      "  -> Saved 6 unique districts for 'Puducherry' to 'Puducherry_districts.txt'\n",
      "  -> Saved 28 unique districts for 'Punjab' to 'Punjab_districts.txt'\n",
      "  -> Saved 46 unique districts for 'Rajasthan' to 'Rajasthan_districts.txt'\n",
      "  -> Saved 10 unique districts for 'Sikkim' to 'Sikkim_districts.txt'\n",
      "  -> Saved 46 unique districts for 'Tamil Nadu' to 'Tamil_Nadu_districts.txt'\n",
      "  -> Saved 43 unique districts for 'Telangana' to 'Telangana_districts.txt'\n",
      "  -> Saved 9 unique districts for 'Tripura' to 'Tripura_districts.txt'\n",
      "  -> Saved 96 unique districts for 'Uttar Pradesh' to 'Uttar_Pradesh_districts.txt'\n",
      "  -> Saved 16 unique districts for 'Uttarakhand' to 'Uttarakhand_districts.txt'\n",
      "  -> Saved 66 unique districts for 'West Bengal' to 'West_Bengal_districts.txt'\n",
      "\n",
      "--- Process Complete ---\n",
      "Successfully processed and created files for 36 states/UTs.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "def extract_districts_by_state():\n",
    "    \"\"\"\n",
    "    Reads cleaned level-2 data, groups by state, and saves a unique,\n",
    "    sorted list of districts for each state into a separate text file.\n",
    "    \"\"\"\n",
    "    # Define source and destination directories\n",
    "    level2_dir = os.path.join('data', 'level2_processing')\n",
    "    output_dir = os.path.join('state_wise_districts')\n",
    "\n",
    "    # --- 1. Setup and Pre-checks ---\n",
    "\n",
    "    # Check if the source directory exists\n",
    "    if not os.path.isdir(level2_dir):\n",
    "        print(f\"Error: The source directory '{level2_dir}' was not found.\")\n",
    "        print(\"Please ensure you have run the previous cleaning script to generate the level-2 files.\")\n",
    "        return\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"Output will be saved in '{output_dir}' directory.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error: Could not create directory {output_dir}: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Read and Combine all Level-2 Data ---\n",
    "    \n",
    "    files_to_process = [\n",
    "        'processed_biometric_data.csv',\n",
    "        'processed_demographic_data.csv',\n",
    "        'processed_enrolment_data.csv'\n",
    "    ]\n",
    "    \n",
    "    all_dataframes = []\n",
    "    print(\"\\nReading and combining level-2 files...\")\n",
    "\n",
    "    for filename in files_to_process:\n",
    "        file_path = os.path.join(level2_dir, filename)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            all_dataframes.append(df)\n",
    "            print(f\"  Successfully loaded '{filename}'.\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"  Warning: File not found: '{filename}'. Skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  An error occurred reading {filename}: {e}\")\n",
    "\n",
    "    if not all_dataframes:\n",
    "        print(\"\\nError: No data could be loaded. Aborting process.\")\n",
    "        return\n",
    "\n",
    "    # Combine all loaded data into a single DataFrame\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    print(f\"Combined data contains {len(combined_df)} total rows.\")\n",
    "\n",
    "    # --- 3. Identify Columns and Clean Data ---\n",
    "\n",
    "    # Find the state and district columns robustly\n",
    "    state_col = next((col for col in ['state', 'State'] if col in combined_df.columns), None)\n",
    "    district_col = next((col for col in ['district', 'District'] if col in combined_df.columns), None)\n",
    "\n",
    "    if not state_col or not district_col:\n",
    "        print(\"\\nError: Could not find 'state' or 'district' columns in the combined data. Aborting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Using '{state_col}' for states and '{district_col}' for districts.\")\n",
    "\n",
    "    # Drop rows where state or district is missing\n",
    "    original_rows = len(combined_df)\n",
    "    combined_df.dropna(subset=[state_col, district_col], inplace=True)\n",
    "    print(f\"Removed {original_rows - len(combined_df)} rows with missing state or district info.\")\n",
    "    \n",
    "    # --- 4. Group by State and Extract Districts ---\n",
    "    \n",
    "    print(\"\\nProcessing data for each state...\")\n",
    "    # Group the DataFrame by the state column\n",
    "    grouped_by_state = combined_df.groupby(state_col)\n",
    "    \n",
    "    states_processed_count = 0\n",
    "    for state_name, group_df in grouped_by_state:\n",
    "        # Get unique district names and sort them alphabetically\n",
    "        unique_districts = sorted(group_df[district_col].astype(str).unique())\n",
    "\n",
    "        # Create a safe and valid filename for the text file\n",
    "        # Replaces spaces with underscores and removes other invalid characters\n",
    "        safe_filename = re.sub(r'[^a-zA-Z0-9_]', '', state_name.replace(' ', '_').replace('&', 'and'))\n",
    "        output_filename = f\"{safe_filename}_districts.txt\"\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "        try:\n",
    "            # Write the list of districts to the new text file\n",
    "            with open(output_path, 'w') as f:\n",
    "                for district in unique_districts:\n",
    "                    f.write(f\"{district}\\n\")\n",
    "            \n",
    "            print(f\"  -> Saved {len(unique_districts)} unique districts for '{state_name}' to '{output_filename}'\")\n",
    "            states_processed_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"  -> Error writing file for '{state_name}': {e}\")\n",
    "            \n",
    "    print(f\"\\n--- Process Complete ---\")\n",
    "    print(f\"Successfully processed and created files for {states_processed_count} states/UTs.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_districts_by_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22bb165d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GeoJSON file: 'data\\india_districts_simplified.geojson'\n",
      "Successfully loaded the GeoJSON data.\n",
      "Extracting state and district names...\n",
      "Cleaning and sorting the district lists...\n",
      "\n",
      "--- Process Complete ---\n",
      "Successfully saved the state-district list to 'state_districts_list.json'.\n",
      "Features with missing data have been grouped under the 'Unknown' key.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def create_state_district_list_with_unknowns():\n",
    "    \"\"\"\n",
    "    Finds a GeoJSON file, extracts state and district names, and groups\n",
    "    any features with missing data under an 'Unknown' category instead of skipping them.\n",
    "    \"\"\"\n",
    "    data_dir = 'data'\n",
    "    output_filename = os.path.join('state_districts_list.json')\n",
    "    \n",
    "    # --- 1. Find the GeoJSON file ---\n",
    "    geojson_file_path = None\n",
    "    try:\n",
    "        for filename in os.listdir(data_dir):\n",
    "            if filename.lower().endswith('.geojson'):\n",
    "                geojson_file_path = os.path.join(data_dir, filename)\n",
    "                print(f\"Found GeoJSON file: '{geojson_file_path}'\")\n",
    "                break\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The directory '{data_dir}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    if not geojson_file_path:\n",
    "        print(f\"Error: No .geojson file found in the '{data_dir}' directory.\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Read the GeoJSON file ---\n",
    "    try:\n",
    "        with open(geojson_file_path, 'r') as f:\n",
    "            geojson_data = json.load(f)\n",
    "        print(\"Successfully loaded the GeoJSON data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Extract States and Districts, Handling Missing Data ---\n",
    "    \n",
    "    state_district_map = defaultdict(list)\n",
    "    print(\"Extracting state and district names...\")\n",
    "\n",
    "    if 'features' in geojson_data and isinstance(geojson_data['features'], list):\n",
    "        for feature in geojson_data['features']:\n",
    "            properties = feature.get('properties')\n",
    "            \n",
    "            # Check if the 'properties' object itself is missing or not a dictionary\n",
    "            if not isinstance(properties, dict):\n",
    "                # This is a severely malformed feature, group it under Unknown.\n",
    "                state_district_map['Unknown'].append('Unknown District (No Properties)')\n",
    "                continue\n",
    "\n",
    "            # .get() is used to provide a default value if a key is missing.\n",
    "            # This is the core change to prevent skipping.\n",
    "            state_name = properties.get('st_nm', 'Unknown')\n",
    "            district_name = properties.get('district', 'Unknown District (Missing Name)')\n",
    "            \n",
    "            # If the state name was missing, we map it to the 'Unknown' key.\n",
    "            # If the district name was missing, its default value will be added.\n",
    "            state_district_map[state_name].append(district_name)\n",
    "    \n",
    "    # --- 4. Clean up the data ---\n",
    "    print(\"Cleaning and sorting the district lists...\")\n",
    "    final_data = {}\n",
    "    for state, districts in state_district_map.items():\n",
    "        # Using set() removes duplicates, and sorted() puts them in order.\n",
    "        final_data[state] = sorted(list(set(districts)))\n",
    "\n",
    "    # --- 5. Save the result to a new JSON file ---\n",
    "    try:\n",
    "        with open(output_filename, 'w') as f:\n",
    "            json.dump(final_data, f, indent=4)\n",
    "        print(f\"\\n--- Process Complete ---\")\n",
    "        print(f\"Successfully saved the state-district list to '{output_filename}'.\")\n",
    "        print(\"Features with missing data have been grouped under the 'Unknown' key.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing the output file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_state_district_list_with_unknowns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43f2fb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'data\\level3_processing' is ready.\n",
      "\n",
      "--- Processing 'processed_biometric_data.csv' ---\n",
      "Before cleaning, row count: 1861108\n",
      "After cleaning, row count is: 1571972\n",
      "Rows removed: 289136\n",
      "Successfully saved cleaned file to 'data\\level3_processing\\processed_biometric_data.csv'\n",
      "\n",
      "--- Processing 'processed_demographic_data.csv' ---\n",
      "Before cleaning, row count: 2071687\n",
      "After cleaning, row count is: 1739974\n",
      "Rows removed: 331713\n",
      "Successfully saved cleaned file to 'data\\level3_processing\\processed_demographic_data.csv'\n",
      "\n",
      "--- Processing 'processed_enrolment_data.csv' ---\n",
      "Before cleaning, row count: 1006007\n",
      "After cleaning, row count is: 875852\n",
      "Rows removed: 130155\n",
      "Successfully saved cleaned file to 'data\\level3_processing\\processed_enrolment_data.csv'\n",
      "\n",
      "--- All files processed. Level 3 data is ready. ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# This dictionary is the \"source of truth\". It contains every state and its\n",
    "# final, officially validated list of districts.\n",
    "OFFICIAL_DISTRICTS_PER_STATE = {\n",
    "    'Andaman and Nicobar Islands': ['Nicobars', 'North and Middle Andaman', 'South Andaman'],\n",
    "    'Andhra Pradesh': ['Alluri Sitharama Raju', 'Anakapalli', 'Ananthapuramu', 'Annamayya', 'Bapatla', 'Chittoor', 'Dr. B.R. Ambedkar Konaseema', 'East Godavari', 'Eluru', 'Guntur', 'Kakinada', 'Krishna', 'Kurnool', 'N.T.R.', 'Nandyal', 'Palnadu', 'Parvathipuram Manyam', 'Prakasam', 'S.P.S. Nellore', 'Sri Sathya Sai', 'Srikakulam', 'Tirupati', 'Visakhapatnam', 'Vizianagaram', 'West Godavari', 'Y.S.R.'],\n",
    "    'Arunachal Pradesh': ['Anjaw', 'Changlang', 'Dibang Valley', 'East Kameng', 'East Siang', 'Kamle', 'Kra Daadi', 'Kurung Kumey', 'Lepa Rada', 'Lohit', 'Longding', 'Lower Dibang Valley', 'Lower Siang', 'Lower Subansiri', 'Namsai', 'Pakke Kessang', 'Papum Pare', 'Shi Yomi', 'Siang', 'Tawang', 'Tirap', 'Upper Siang', 'Upper Subansiri', 'West Kameng', 'West Siang'],\n",
    "    'Assam': ['Bajali', 'Baksa', 'Barpeta', 'Biswanath', 'Bongaigaon', 'Cachar', 'Charaideo', 'Chirang', 'Darrang', 'Dhemaji', 'Dhubri', 'Dibrugarh', 'Dima Hasao', 'Goalpara', 'Golaghat', 'Hailakandi', 'Hojai', 'Jorhat', 'Kamrup', 'Kamrup Metropolitan', 'Karbi Anglong', 'Karimganj', 'Kokrajhar', 'Lakhimpur', 'Majuli', 'Morigaon', 'Nagaon', 'Nalbari', 'Sivasagar', 'Sonitpur', 'South Salmara-Mankachar', 'Tamulpur', 'Tinsukia', 'Udalguri', 'West Karbi Anglong'],\n",
    "    'Bihar': ['Araria', 'Arwal', 'Aurangabad', 'Banka', 'Begusarai', 'Bhagalpur', 'Bhojpur', 'Buxar', 'Darbhanga', 'East Champaran', 'Gaya', 'Gopalganj', 'Jamui', 'Jehanabad', 'Kaimur', 'Katihar', 'Khagaria', 'Kishanganj', 'Lakhisarai', 'Madhepura', 'Madhubani', 'Munger', 'Muzaffarpur', 'Nalanda', 'Nawada', 'Patna', 'Purnia', 'Rohtas', 'Saharsa', 'Samastipur', 'Saran', 'Sheikhpura', 'Sheohar', 'Sitamarhi', 'Siwan', 'Supaul', 'Vaishali', 'West Champaran'],\n",
    "    'Chandigarh': ['Chandigarh'],\n",
    "    'Chhattisgarh': ['Balod', 'Baloda Bazar', 'Balrampur', 'Bastar', 'Bemetara', 'Bijapur', 'Bilaspur', 'Dantewada', 'Dhamtari', 'Durg', 'Gariyaband', 'Gaurela-Pendra-Marwahi', 'Janjgir-Champa', 'Jashpur', 'Kabirdham', 'Kanker', 'Khairagarh-Chhuikhadan-Gandai', 'Kondagaon', 'Korba', 'Koriya', 'Mahasamund', 'Manendragarh-Chirmiri-Bharatpur', 'Mohla-Manpur-Ambagarh Chowki', 'Mungeli', 'Narayanpur', 'Raigarh', 'Raipur', 'Rajnandgaon', 'Sakti', 'Sarangarh-Bilaigarh', 'Sukma', 'Surajpur', 'Surguja'],\n",
    "    'Dadra and Nagar Haveli and Daman and Diu': ['Dadra and Nagar Haveli', 'Daman', 'Diu'],\n",
    "    'Delhi': ['Central Delhi', 'East Delhi', 'New Delhi', 'North Delhi', 'North East Delhi', 'North West Delhi', 'Shahdara', 'South Delhi', 'South East Delhi', 'South West Delhi', 'West Delhi'],\n",
    "    'Goa': ['North Goa', 'South Goa'],\n",
    "    'Gujarat': ['Ahmedabad', 'Amreli', 'Anand', 'Aravalli', 'Banaskantha', 'Bharuch', 'Bhavnagar', 'Botad', 'Chhota Udepur', 'Dahod', 'Dang', 'Devbhoomi Dwarka', 'Gandhinagar', 'Gir Somnath', 'Jamnagar', 'Junagadh', 'Kutch', 'Kheda', 'Mahisagar', 'Mehsana', 'Morbi', 'Narmada', 'Navsari', 'Panchmahal', 'Patan', 'Porbandar', 'Rajkot', 'Sabarkantha', 'Surat', 'Surendranagar', 'Tapi', 'Vadodara', 'Valsad'],\n",
    "    'Haryana': ['Ambala', 'Bhiwani', 'Charkhi Dadri', 'Faridabad', 'Fatehabad', 'Gurugram', 'Hisar', 'Jhajjar', 'Jind', 'Kaithal', 'Karnal', 'Kurukshetra', 'Mahendragarh', 'Nuh', 'Palwal', 'Panchkula', 'Panipat', 'Rewari', 'Rohtak', 'Sirsa', 'Sonipat', 'Yamunanagar'],\n",
    "    'Himachal Pradesh': ['Bilaspur', 'Chamba', 'Hamirpur', 'Kangra', 'Kinnaur', 'Kullu', 'Lahaul and Spiti', 'Mandi', 'Shimla', 'Sirmaur', 'Solan', 'Una'],\n",
    "    'Jammu and Kashmir': ['Anantnag', 'Bandipora', 'Baramulla', 'Budgam', 'Doda', 'Ganderbal', 'Jammu', 'Kathua', 'Kishtwar', 'Kulgam', 'Kupwara', 'Poonch', 'Pulwama', 'Rajouri', 'Ramban', 'Reasi', 'Samba', 'Shopian', 'Srinagar', 'Udhampur'],\n",
    "    'Jharkhand': ['Bokaro', 'Chatra', 'Deoghar', 'Dhanbad', 'Dumka', 'East Singhbhum', 'Garhwa', 'Giridih', 'Godda', 'Gumla', 'Hazaribagh', 'Jamtara', 'Khunti', 'Koderma', 'Latehar', 'Lohardaga', 'Pakur', 'Palamu', 'Ramgarh', 'Ranchi', 'Sahibganj', 'Seraikela Kharsawan', 'Simdega', 'West Singhbhum'],\n",
    "    'Karnataka': ['Bagalkot', 'Ballari', 'Belagavi', 'Bengaluru Rural', 'Bengaluru Urban', 'Bidar', 'Chamarajanagar', 'Chikkaballapur', 'Chikkamagaluru', 'Chitradurga', 'Dakshina Kannada', 'Davanagere', 'Dharwad', 'Gadag', 'Hassan', 'Haveri', 'Kalaburagi', 'Kodagu', 'Kolar', 'Koppal', 'Mandya', 'Mysuru', 'Raichur', 'Ramanagara', 'Shivamogga', 'Tumakuru', 'Udupi', 'Uttara Kannada', 'Vijayanagara', 'Vijayapura', 'Yadgir'],\n",
    "    'Kerala': ['Alappuzha', 'Ernakulam', 'Idukki', 'Kannur', 'Kasaragod', 'Kollam', 'Kottayam', 'Kozhikode', 'Malappuram', 'Palakkad', 'Pathanamthitta', 'Thiruvananthapuram', 'Thrissur', 'Wayanad'],\n",
    "    'Ladakh': ['Kargil', 'Leh'],\n",
    "    'Lakshadweep': ['Lakshadweep'],\n",
    "    'Madhya Pradesh': ['Agar Malwa', 'Alirajpur', 'Anuppur', 'Ashoknagar', 'Balaghat', 'Barwani', 'Betul', 'Bhind', 'Bhopal', 'Burhanpur', 'Chhatarpur', 'Chhindwara', 'Damoh', 'Datia', 'Dewas', 'Dhar', 'Dindori', 'Guna', 'Gwalior', 'Harda', 'Indore', 'Jabalpur', 'Jhabua', 'Katni', 'Khandwa', 'Khargone', 'Maihar', 'Mandla', 'Mandsaur', 'Mauganj', 'Morena', 'Narmadapuram', 'Narsinghpur', 'Neemuch', 'Niwari', 'Pandhurna', 'Panna', 'Raisen', 'Rajgarh', 'Ratlam', 'Rewa', 'Sagar', 'Satna', 'Sehore', 'Seoni', 'Shahdol', 'Shajapur', 'Sheopur', 'Shivpuri', 'Sidhi', 'Singrauli', 'Tikamgarh', 'Ujjain', 'Umaria', 'Vidisha'],\n",
    "    'Maharashtra': ['Ahmednagar', 'Akola', 'Amravati', 'Beed', 'Bhandara', 'Buldhana', 'Chandrapur', 'Chhatrapati Sambhajinagar', 'Dharashiv', 'Dhule', 'Gadchiroli', 'Gondia', 'Hingoli', 'Jalgaon', 'Jalna', 'Kolhapur', 'Latur', 'Mumbai City', 'Mumbai Suburban', 'Nagpur', 'Nanded', 'Nandurbar', 'Nashik', 'Palghar', 'Parbhani', 'Pune', 'Raigad', 'Ratnagiri', 'Sangli', 'Satara', 'Sindhudurg', 'Solapur', 'Thane', 'Wardha', 'Washim', 'Yavatmal'],\n",
    "    'Manipur': ['Bishnupur', 'Chandel', 'Churachandpur', 'Imphal East', 'Imphal West', 'Jiribam', 'Kakching', 'Kangpokpi', 'Pherzawl', 'Senapati', 'Tamenglong', 'Thoubal', 'Ukhrul'],\n",
    "    'Meghalaya': ['East Garo Hills', 'East Jaintia Hills', 'East Khasi Hills', 'Eastern West Khasi Hills', 'North Garo Hills', 'Ri-Bhoi', 'South Garo Hills', 'South West Garo Hills', 'South West Khasi Hills', 'West Garo Hills', 'West Jaintia Hills', 'West Khasi Hills'],\n",
    "    'Mizoram': ['Aizawl', 'Champhai', 'Hnahthial', 'Khawzawl', 'Kolasib', 'Lawngtlai', 'Lunglei', 'Mamit', 'Saiha', 'Saitual', 'Serchhip'],\n",
    "    'Nagaland': ['Chümoukedima', 'Dimapur', 'Kiphire', 'Kohima', 'Longleng', 'Mokokchung', 'Mon', 'Niuland', 'Noklak', 'Peren', 'Phek', 'Shamator', 'Tseminyü', 'Tuensang', 'Wokha', 'Zunheboto'],\n",
    "    'Odisha': ['Angul', 'Balangir', 'Balasore', 'Bargarh', 'Bhadrak', 'Boudh', 'Cuttack', 'Deogarh', 'Dhenkanal', 'Gajapati', 'Ganjam', 'Jagatsinghpur', 'Jajpur', 'Jharsuguda', 'Kalahandi', 'Kandhamal', 'Kendrapara', 'Kendujhar', 'Khordha', 'Koraput', 'Malkangiri', 'Mayurbhanj', 'Nabarangpur', 'Nayagarh', 'Nuapada', 'Puri', 'Rayagada', 'Sambalpur', 'Subarnapur', 'Sundargarh'],\n",
    "    'Puducherry': ['Karaikal', 'Mahé', 'Puducherry', 'Yanam'],\n",
    "    'Punjab': ['Amritsar', 'Barnala', 'Bathinda', 'Faridkot', 'Fatehgarh Sahib', 'Fazilka', 'Ferozepur', 'Gurdaspur', 'Hoshiarpur', 'Jalandhar', 'Kapurthala', 'Ludhiana', 'Malerkotla', 'Mansa', 'Moga', 'Pathankot', 'Patiala', 'Rupnagar', 'Sahibzada Ajit Singh Nagar', 'Sangrur', 'Shaheed Bhagat Singh Nagar', 'Sri Muktsar Sahib', 'Tarn Taran'],\n",
    "    'Rajasthan': ['Ajmer', 'Alwar', 'Balotra', 'Banswara', 'Baran', 'Barmer', 'Beawar', 'Bharatpur', 'Bhilwara', 'Bikaner', 'Bundi', 'Chittorgarh', 'Churu', 'Dausa', 'Deeg', 'Dholpur', 'Didwana-Kuchaman', 'Dungarpur', 'Hanumangarh', 'Jaipur', 'Jaisalmer', 'Jalore', 'Jhalawar', 'Jhunjhunu', 'Jodhpur', 'Karauli', 'Khairthal-Tijara', 'Kota', 'Kotputli-Behror', 'Nagaur', 'Pali', 'Phalodi', 'Pratapgarh', 'Rajsamand', 'Salumbar', 'Sawai Madhopur', 'Sikar', 'Sirohi', 'Sri Ganganagar', 'Tonk', 'Udaipur'],\n",
    "    'Sikkim': ['Gangtok', 'Gyalshing', 'Mangan', 'Namchi'],\n",
    "    'Tamil Nadu': ['Ariyalur', 'Chengalpattu', 'Chennai', 'Coimbatore', 'Cuddalore', 'Dharmapuri', 'Dindigul', 'Erode', 'Kallakurichi', 'Kancheepuram', 'Kanniyakumari', 'Karur', 'Krishnagiri', 'Madurai', 'Mayiladuthurai', 'Nagapattinam', 'Namakkal', 'Perambalur', 'Pudukkottai', 'Ramanathapuram', 'Ranipet', 'Salem', 'Sivaganga', 'Tenkasi', 'Thanjavur', 'The Nilgiris', 'Theni', 'Thiruvarur', 'Thoothukudi', 'Tiruchirappalli', 'Tirunelveli', 'Tirupattur', 'Tiruppur', 'Tiruvallur', 'Tiruvannamalai', 'Vellore', 'Viluppuram', 'Virudhunagar'],\n",
    "    'Telangana': ['Adilabad', 'Bhadradri Kothagudem', 'Hanumakonda', 'Hyderabad', 'Jagitial', 'Jangaon', 'Jayashankar Bhupalpally', 'Jogulamba Gadwal', 'Kamareddy', 'Karimnagar', 'Khammam', 'Komaram Bheem', 'Mahabubabad', 'Mahabubnagar', 'Mancherial', 'Medak', 'Medchal-Malkajgiri', 'Mulugu', 'Nagarkurnool', 'Nalgonda', 'Narayanpet', 'Nirmal', 'Nizamabad', 'Peddapalli', 'Rajanna Sircilla', 'Rangareddy', 'Sangareddy', 'Siddipet', 'Suryapet', 'Vikarabad', 'Wanaparthy', 'Warangal', 'Yadadri Bhuvanagiri'],\n",
    "    'Tripura': ['Dhalai', 'Gomati', 'Khowai', 'North Tripura', 'Sepahijala', 'South Tripura', 'Unakoti', 'West Tripura'],\n",
    "    'Uttar Pradesh': ['Agra', 'Aligarh', 'Ambedkar Nagar', 'Amethi', 'Amroha', 'Auraiya', 'Ayodhya', 'Azamgarh', 'Baghpat', 'Bahraich', 'Ballia', 'Balrampur', 'Banda', 'Barabanki', 'Bareilly', 'Basti', 'Bhadohi', 'Bijnor', 'Budaun', 'Bulandshahr', 'Chandauli', 'Chitrakoot', 'Deoria', 'Etah', 'Etawah', 'Farrukhabad', 'Fatehpur', 'Firozabad', 'Gautam Buddha Nagar', 'Ghaziabad', 'Ghazipur', 'Gonda', 'Gorakhpur', 'Hamirpur', 'Hapur', 'Hardoi', 'Hathras', 'Jalaun', 'Jaunpur', 'Jhansi', 'Kannauj', 'Kanpur Dehat', 'Kanpur Nagar', 'Kasganj', 'Kaushambi', 'Kushinagar', 'Lakhimpur Kheri', 'Lalitpur', 'Lucknow', 'Maharajganj', 'Mahoba', 'Mainpuri', 'Mathura', 'Mau', 'Meerut', 'Mirzapur', 'Moradabad', 'Muzaffarnagar', 'Pilibhit', 'Pratapgarh', 'Prayagraj', 'Raebareli', 'Rampur', 'Saharanpur', 'Sambhal', 'Sant Kabir Nagar', 'Shahjahanpur', 'Shamli', 'Shravasti', 'Siddharthnagar', 'Sitapur', 'Sonbhadra', 'Sultanpur', 'Unnao', 'Varanasi'],\n",
    "    'Uttarakhand': ['Almora', 'Bageshwar', 'Chamoli', 'Champawat', 'Dehradun', 'Haridwar', 'Nainital', 'Pauri Garhwal', 'Pithoragarh', 'Rudraprayag', 'Tehri Garhwal', 'Udham Singh Nagar', 'Uttarkashi'],\n",
    "    'West Bengal': ['Alipurduar', 'Bankura', 'Birbhum', 'Cooch Behar', 'Dakshin Dinajpur', 'Darjeeling', 'Hooghly', 'Howrah', 'Jalpaiguri', 'Jhargram', 'Kalimpong', 'Kolkata', 'Malda', 'Murshidabad', 'Nadia', 'North 24 Parganas', 'Paschim Bardhaman', 'Paschim Medinipur', 'Purba Bardhaman', 'Purba Medinipur', 'Purulia', 'South 24 Parganas', 'Uttar Dinajpur']\n",
    "}\n",
    "\n",
    "# --- MASTER CORRECTION MAP ---\n",
    "# This dictionary maps known variants (lowercase) to the official names.\n",
    "# This is a sample; a full one would be built from our entire conversation.\n",
    "MASTER_CORRECTION_MAP = {\n",
    "    # Andhra Pradesh / Telangana\n",
    "    \"anantapur\": \"Ananthapuramu\", \"cuddapah\": \"Y.S.R.\", \"spsr nellore\": \"S.P.S. Nellore\",\n",
    "    \"k.v.rangareddy\": \"Rangareddy\", \"ranga reddy\": \"Rangareddy\",\n",
    "    # Haryana\n",
    "    \"gurgaon\": \"Gurugram\", \"mewat\": \"Nuh\",\n",
    "    # Uttar Pradesh\n",
    "    \"allahabad\": \"Prayagraj\", \"faizabad\": \"Ayodhya\", \"jyotiba phule nagar\": \"Amroha\",\n",
    "    # Maharashtra\n",
    "    \"aurangabad\": \"Chhatrapati Sambhajinagar\", \"osmanabad\": \"Dharashiv\",\n",
    "    # Karnataka\n",
    "    \"bangalore\": \"Bengaluru Urban\", \"belgaum\": \"Belagavi\", \"gulbarga\": \"Kalaburagi\", \"mysore\": \"Mysuru\",\n",
    "    # West Bengal\n",
    "    \"barddhaman\": \"Purba Bardhaman\", \"burdwan\": \"Purba Bardhaman\", # Note: Ambiguous, mapping to one for simplicity\n",
    "    \"darjiling\": \"Darjeeling\",\n",
    "    # Add all other mappings from our conversation here...\n",
    "}\n",
    "\n",
    "\n",
    "def clean_dataframe(df, state_col, district_col):\n",
    "    \"\"\"Applies the cleaning logic to an entire DataFrame.\"\"\"\n",
    "    \n",
    "    # Create a quick-lookup set of official districts for each state (all lowercase)\n",
    "    official_districts_lower = defaultdict(set)\n",
    "    for state, districts in OFFICIAL_DISTRICTS_PER_STATE.items():\n",
    "        official_districts_lower[state] = {d.lower() for d in districts}\n",
    "\n",
    "    # --- Cleaning Function to be applied to each row ---\n",
    "    def get_corrected_district(row):\n",
    "        state = row[state_col]\n",
    "        district_original = str(row[district_col]).strip()\n",
    "        district_lower = district_original.lower()\n",
    "\n",
    "        # Case 1: The district is in our direct correction map\n",
    "        if district_lower in MASTER_CORRECTION_MAP:\n",
    "            return MASTER_CORRECTION_MAP[district_lower]\n",
    "\n",
    "        # Case 2: The district is already official for that state\n",
    "        if state in official_districts_lower and district_lower in official_districts_lower[state]:\n",
    "            return district_original # It's already valid\n",
    "            \n",
    "        # Case 3: The district is invalid/unmapped\n",
    "        return None\n",
    "\n",
    "    # Apply the function to get a new series of cleaned district names\n",
    "    cleaned_districts = df.apply(get_corrected_district, axis=1)\n",
    "    \n",
    "    # Create a copy to avoid SettingWithCopyWarning\n",
    "    df_cleaned = df.copy()\n",
    "    df_cleaned['district'] = cleaned_districts\n",
    "    \n",
    "    # Drop rows where the district was invalid (returned None)\n",
    "    df_cleaned.dropna(subset=['district'], inplace=True)\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "def create_level3_files():\n",
    "    \"\"\"Main function to process level-2 files and create cleaned level-3 files.\"\"\"\n",
    "    \n",
    "    # Define source and destination directories\n",
    "    level2_dir = os.path.join('data', 'level2_processing')\n",
    "    level3_dir = os.path.join('data', 'level3_processing')\n",
    "\n",
    "    # Create the level-3 directory\n",
    "    os.makedirs(level3_dir, exist_ok=True)\n",
    "    print(f\"Directory '{level3_dir}' is ready.\")\n",
    "\n",
    "    files_to_process = [\n",
    "        'processed_biometric_data.csv',\n",
    "        'processed_demographic_data.csv',\n",
    "        'processed_enrolment_data.csv'\n",
    "    ]\n",
    "\n",
    "    # Find the state and district column names (assuming they are consistent)\n",
    "    # You may need to adjust these if column names vary per file\n",
    "    state_column = 'state'\n",
    "    district_column = 'district'\n",
    "\n",
    "    for filename in files_to_process:\n",
    "        input_path = os.path.join(level2_dir, filename)\n",
    "        output_path = os.path.join(level3_dir, filename)\n",
    "\n",
    "        if not os.path.exists(input_path):\n",
    "            print(f\"\\n--- Warning: File '{filename}' not found. Skipping. ---\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- Processing '{filename}' ---\")\n",
    "        df = pd.read_csv(input_path)\n",
    "        \n",
    "        # Report \"Before\" count\n",
    "        before_count = len(df)\n",
    "        print(f\"Before cleaning, row count: {before_count}\")\n",
    "\n",
    "        # Perform the cleaning\n",
    "        cleaned_df = clean_dataframe(df, state_column, district_column)\n",
    "        \n",
    "        # Report \"After\" count\n",
    "        after_count = len(cleaned_df)\n",
    "        print(f\"After cleaning, row count is: {after_count}\")\n",
    "        print(f\"Rows removed: {before_count - after_count}\")\n",
    "\n",
    "        # Save the cleaned file\n",
    "        cleaned_df.to_csv(output_path, index=False)\n",
    "        print(f\"Successfully saved cleaned file to '{output_path}'\")\n",
    "\n",
    "    print(\"\\n--- All files processed. Level 3 data is ready. ---\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_level3_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fc8b0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'data\\level3_processing' is ready.\n",
      "\n",
      "--- Harmonizing 'processed_biometric_data.csv' ---\n",
      "Before harmonization, row count: 1861108\n",
      "After harmonization, row count is: 1521410\n",
      "Rows removed: 339698\n",
      "Successfully saved harmonized file to 'data\\level3_processing\\processed_biometric_data.csv'\n",
      "\n",
      "--- Harmonizing 'processed_demographic_data.csv' ---\n",
      "Before harmonization, row count: 2071687\n",
      "After harmonization, row count is: 1669920\n",
      "Rows removed: 401767\n",
      "Successfully saved harmonized file to 'data\\level3_processing\\processed_demographic_data.csv'\n",
      "\n",
      "--- Harmonizing 'processed_enrolment_data.csv' ---\n",
      "Before harmonization, row count: 1006007\n",
      "After harmonization, row count is: 850291\n",
      "Rows removed: 155716\n",
      "Successfully saved harmonized file to 'data\\level3_processing\\processed_enrolment_data.csv'\n",
      "\n",
      "--- All files harmonized. Level 3 data is ready for plotting. ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "def load_geojson_districts(filepath):\n",
    "    \"\"\"Loads the user-provided JSON file as the source of truth.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"FATAL ERROR: The required file '{filepath}' was not found.\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"FATAL ERROR: The file '{filepath}' is not a valid JSON file.\")\n",
    "        return None\n",
    "\n",
    "def build_correction_map(geojson_map):\n",
    "    \"\"\"\n",
    "    Builds a master map to correct all known variations to match the GeoJSON names.\n",
    "    This is the \"brain\" of the operation.\n",
    "    \"\"\"\n",
    "    # Key: messy name (lowercase). Value: Target name (from GeoJSON).\n",
    "    correction_map = {}\n",
    "    \n",
    "    # --- Data from our extensive conversation, mapped to GeoJSON targets ---\n",
    "\n",
    "    # Name changes where GeoJSON uses the NEW name\n",
    "    correction_map.update({\n",
    "        'gurgaon': 'Gurugram', 'mewat': 'Nuh', 'allahabad': 'Prayagraj', 'faizabad': 'Ayodhya',\n",
    "        'belgaum': 'Belagavi', 'gulgulbarga': 'Kalaburagi', 'mysore': 'Mysuru',\n",
    "        'shimoga': 'Shivamogga', 'tumkur': 'Tumakuru', 'bijapur': 'Vijayapura',\n",
    "        'hoshangabad': 'Narmadapuram', # Note: Your JSON has Hoshangabad, this will be handled later\n",
    "        'nawanshahr': 'Shahid Bhagat Singh Nagar'\n",
    "    })\n",
    "\n",
    "    # Name changes where GeoJSON uses the OLD name\n",
    "    correction_map.update({\n",
    "        'dharashiv': 'Osmanabad', 'chhatrapati sambhajinagar': 'Aurangabad'\n",
    "    })\n",
    "\n",
    "    # Spelling fixes and variations\n",
    "    correction_map.update({\n",
    "        'firozpur': 'Ferozepur', 's.a.s. nagar': 'S.A.S. Nagar', 's.a.s nagar(mohali)': 'S.A.S. Nagar',\n",
    "        'sri muktsar sahib': 'Sri Muktsar Sahib', 'muktsar': 'Sri Muktsar Sahib',\n",
    "        'darjiling': 'Darjeeling', 'koch bihar': 'Cooch Behar', 'hugli': 'Hooghly',\n",
    "        'puruliya': 'Purulia', 'medinipur': None, # Ambiguous, must be dropped\n",
    "        'barddhaman': None, 'burdwan': None, # Ambiguous, must be dropped\n",
    "        'k.v.rangareddy': 'Ranga Reddy', 'ranga reddy': 'Ranga Reddy', 'visakhapatanam': 'Visakhapatnam',\n",
    "        'mammit': 'Mamit', 'anugul': 'Angul', 'baleshwar': 'Balasore', 'debargarh': 'Deogarh',\n",
    "        'jajapur': 'Jajpur', 'khorda': 'Khordha', 'subarnapur': 'Subarnapur',\n",
    "        'hardwar': 'Haridwar'\n",
    "    })\n",
    "    \n",
    "    # Cross-check the map against the GeoJSON to resolve conflicts\n",
    "    # Example: Hoshangabad vs Narmadapuram\n",
    "    if 'Madhya Pradesh' in geojson_map and 'Hoshangabad' in geojson_map['Madhya Pradesh']:\n",
    "        correction_map['narmadapuram'] = 'Hoshangabad' # Force mapping to what the GeoJSON expects\n",
    "        correction_map['hoshangabad'] = 'Hoshangabad'\n",
    "    else:\n",
    "        correction_map['hoshangabad'] = 'Narmadapuram' # Default to official if not found\n",
    "\n",
    "    return correction_map\n",
    "\n",
    "def harmonize_files():\n",
    "    \"\"\"Main function to process level-2 files and create harmonized level-3 files.\"\"\"\n",
    "    \n",
    "    data_dir = 'data'\n",
    "    level2_dir = os.path.join(data_dir, 'level2_processing')\n",
    "    level3_dir = os.path.join(data_dir, 'level3_processing')\n",
    "    geojson_path = os.path.join(data_dir, 'state_districts_list.json')\n",
    "\n",
    "    # 1. Load the \"source of truth\"\n",
    "    geojson_districts = load_geojson_districts(geojson_path)\n",
    "    if geojson_districts is None:\n",
    "        return\n",
    "\n",
    "    # Convert lists to sets for fast lookups\n",
    "    geojson_sets = {state: set(districts) for state, districts in geojson_districts.items()}\n",
    "\n",
    "    # 2. Build the correction map\n",
    "    correction_map = build_correction_map(geojson_districts)\n",
    "\n",
    "    # 3. Create the destination directory\n",
    "    os.makedirs(level3_dir, exist_ok=True)\n",
    "    print(f\"Directory '{level3_dir}' is ready.\")\n",
    "\n",
    "    files_to_process = [\n",
    "        'processed_biometric_data.csv',\n",
    "        'processed_demographic_data.csv',\n",
    "        'processed_enrolment_data.csv'\n",
    "    ]\n",
    "    \n",
    "    state_col, district_col = 'state', 'district' # Assuming consistent column names\n",
    "\n",
    "    for filename in files_to_process:\n",
    "        input_path = os.path.join(level2_dir, filename)\n",
    "        if not os.path.exists(input_path):\n",
    "            print(f\"\\n--- Warning: File '{filename}' not found in {level2_dir}. Skipping. ---\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- Harmonizing '{filename}' ---\")\n",
    "        df = pd.read_csv(input_path)\n",
    "        \n",
    "        before_count = len(df)\n",
    "        print(f\"Before harmonization, row count: {before_count}\")\n",
    "\n",
    "        # --- Harmonization Logic ---\n",
    "        def get_harmonized_district(row):\n",
    "            state = row[state_col]\n",
    "            district_original = str(row[district_col]).strip()\n",
    "            district_lower = district_original.lower()\n",
    "\n",
    "            # Step A: Apply direct correction if available\n",
    "            corrected_name = correction_map.get(district_lower, district_original)\n",
    "\n",
    "            if corrected_name is None: # Explicitly marked for dropping (e.g., 'Burdwan')\n",
    "                return None\n",
    "            \n",
    "            # Step B: Validate the name against the GeoJSON list for that state\n",
    "            if state in geojson_sets and corrected_name in geojson_sets[state]:\n",
    "                return corrected_name # Success!\n",
    "            \n",
    "            # If not found, it's invalid\n",
    "            return None\n",
    "\n",
    "        # Apply the logic to the DataFrame\n",
    "        df_copy = df.copy()\n",
    "        df_copy[district_col] = df_copy.apply(get_harmonized_district, axis=1)\n",
    "        \n",
    "        # Drop rows that were not successfully harmonized\n",
    "        df_copy.dropna(subset=[district_col], inplace=True)\n",
    "        \n",
    "        after_count = len(df_copy)\n",
    "        print(f\"After harmonization, row count is: {after_count}\")\n",
    "        print(f\"Rows removed: {before_count - after_count}\")\n",
    "\n",
    "        # Save the harmonized file\n",
    "        output_path = os.path.join(level3_dir, filename)\n",
    "        df_copy.to_csv(output_path, index=False)\n",
    "        print(f\"Successfully saved harmonized file to '{output_path}'\")\n",
    "\n",
    "    print(\"\\n--- All files harmonized. Level 3 data is ready for plotting. ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    harmonize_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc671836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "Andaman and Nicobar\n",
      "Telangana\n",
      "Andhra Pradesh\n",
      "Arunachal Pradesh\n",
      "Assam\n",
      "Bihar\n",
      "Chandigarh\n",
      "Chhattisgarh\n",
      "Dādra and Nagar Haveli and Damān and Diu\n",
      "Delhi\n",
      "Goa\n",
      "Gujarat\n",
      "Haryana\n",
      "Himachal Pradesh\n",
      "Jharkhand\n",
      "Karnataka\n",
      "Kerala\n",
      "Madhya Pradesh\n",
      "Maharashtra\n",
      "Manipur\n",
      "Meghalaya\n",
      "Mizoram\n",
      "Nagaland\n",
      "Orissa\n",
      "Puducherry\n",
      "Punjab\n",
      "Rajasthan\n",
      "Sikkim\n",
      "Tamil Nadu\n",
      "Tripura\n",
      "Uttar Pradesh\n",
      "Uttaranchal\n",
      "West Bengal\n",
      "Lakshadweep\n",
      "Jammu and Kashmir\n",
      "Ladakh\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Andaman and Nicobar',\n",
       " 'Telangana',\n",
       " 'Andhra Pradesh',\n",
       " 'Arunachal Pradesh',\n",
       " 'Assam',\n",
       " 'Bihar',\n",
       " 'Chandigarh',\n",
       " 'Chhattisgarh',\n",
       " 'Dādra and Nagar Haveli and Damān and Diu',\n",
       " 'Delhi',\n",
       " 'Goa',\n",
       " 'Gujarat',\n",
       " 'Haryana',\n",
       " 'Himachal Pradesh',\n",
       " 'Jharkhand',\n",
       " 'Karnataka',\n",
       " 'Kerala',\n",
       " 'Madhya Pradesh',\n",
       " 'Maharashtra',\n",
       " 'Manipur',\n",
       " 'Meghalaya',\n",
       " 'Mizoram',\n",
       " 'Nagaland',\n",
       " 'Orissa',\n",
       " 'Puducherry',\n",
       " 'Punjab',\n",
       " 'Rajasthan',\n",
       " 'Sikkim',\n",
       " 'Tamil Nadu',\n",
       " 'Tripura',\n",
       " 'Uttar Pradesh',\n",
       " 'Uttaranchal',\n",
       " 'West Bengal',\n",
       " 'Lakshadweep',\n",
       " 'Jammu and Kashmir',\n",
       " 'Ladakh']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# If your data is in a file called 'data.json'\n",
    "with open('in.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract all \"name\" values from the properties of each feature\n",
    "names = [feature['properties']['name'] for feature in data['features']]\n",
    "\n",
    "# Print the list of names\n",
    "print(len(names))\n",
    "for name in names:\n",
    "    print(name)\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "943a7d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haaaa\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Starting Level 4 Spatial Cleaning Protocol...\n",
      "\n",
      "Processing processed_biometric_data.csv...\n",
      "   -> Calculating spatial clusters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haaaa\\AppData\\Local\\Temp\\ipykernel_19584\\1796164387.py:73: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_final = df_clean.groupby(['state', 'district'], group_keys=False).apply(filter_spatial_outliers)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Cleaned.\n",
      "   - Original Rows: 1,571,972\n",
      "   - Invalid Pincodes Removed: 40,001\n",
      "   - Spatial Outliers Removed: 134,488\n",
      "   - Final Level 4 Rows: 1,397,483\n",
      "\n",
      "Processing processed_demographic_data.csv...\n",
      "   -> Calculating spatial clusters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haaaa\\AppData\\Local\\Temp\\ipykernel_19584\\1796164387.py:73: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_final = df_clean.groupby(['state', 'district'], group_keys=False).apply(filter_spatial_outliers)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Cleaned.\n",
      "   - Original Rows: 1,739,974\n",
      "   - Invalid Pincodes Removed: 46,691\n",
      "   - Spatial Outliers Removed: 149,695\n",
      "   - Final Level 4 Rows: 1,543,588\n",
      "\n",
      "Processing processed_enrolment_data.csv...\n",
      "   -> Calculating spatial clusters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haaaa\\AppData\\Local\\Temp\\ipykernel_19584\\1796164387.py:73: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_final = df_clean.groupby(['state', 'district'], group_keys=False).apply(filter_spatial_outliers)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Cleaned.\n",
      "   - Original Rows: 875,852\n",
      "   - Invalid Pincodes Removed: 18,264\n",
      "   - Spatial Outliers Removed: 74,765\n",
      "   - Final Level 4 Rows: 782,823\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pgeocode\n",
    "\n",
    "def create_level4_data():\n",
    "    print(\"🔹 Starting Level 4 Spatial Cleaning Protocol...\")\n",
    "    \n",
    "    # 1. Setup Directories\n",
    "    input_dir = 'data/level3_processing'\n",
    "    output_dir = 'data/level4_processing'\n",
    "    if not os.path.exists(output_dir): os.makedirs(output_dir)\n",
    "    \n",
    "    files = [\n",
    "        'processed_biometric_data.csv',\n",
    "        'processed_demographic_data.csv',\n",
    "        'processed_enrolment_data.csv'\n",
    "    ]\n",
    "    \n",
    "    # 2. Initialize Geocoder\n",
    "    nomi = pgeocode.Nominatim('in')\n",
    "    \n",
    "    # 3. Process Files\n",
    "    for filename in files:\n",
    "        input_path = os.path.join(input_dir, filename)\n",
    "        if not os.path.exists(input_path):\n",
    "            print(f\"⚠️ Skipped {filename} (Not found)\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nProcessing {filename}...\")\n",
    "        df = pd.read_csv(input_path)\n",
    "        before_count = len(df)\n",
    "        \n",
    "        # --- A. GEOCODE UNIQUE PINCODES ---\n",
    "        # We only geocode unique pincodes to save time\n",
    "        unique_pins = df['pincode'].unique().astype(str)\n",
    "        geo_data = nomi.query_postal_code(unique_pins)\n",
    "        \n",
    "        # Create a lookup dataframe\n",
    "        pin_map = pd.DataFrame({\n",
    "            'pincode': unique_pins,\n",
    "            'lat': geo_data.latitude,\n",
    "            'lon': geo_data.longitude\n",
    "        })\n",
    "        \n",
    "        # Merge Lat/Lon into main dataframe\n",
    "        df['pincode'] = df['pincode'].astype(str)\n",
    "        df_merged = df.merge(pin_map, on='pincode', how='left')\n",
    "        \n",
    "        # --- B. REMOVE INVALID PINCODES (No Lat/Lon) ---\n",
    "        df_clean = df_merged.dropna(subset=['lat', 'lon'])\n",
    "        invalid_count = before_count - len(df_clean)\n",
    "        \n",
    "        # --- C. SPATIAL OUTLIER DETECTION (The \"100km\" Fix) ---\n",
    "        # For each district, calculate the center. Remove points > 2 Std Devs away.\n",
    "        def filter_spatial_outliers(group):\n",
    "            if len(group) < 5: return group # Too small to filter\n",
    "            \n",
    "            lat_mean, lat_std = group['lat'].mean(), group['lat'].std()\n",
    "            lon_mean, lon_std = group['lon'].mean(), group['lon'].std()\n",
    "            \n",
    "            # If std is tiny (all points in one spot), keep them\n",
    "            if lat_std < 0.05 or lon_std < 0.05: return group\n",
    "            \n",
    "            # Keep points within 2 Sigma (95% confidence)\n",
    "            # This deletes the \"Mumbai pincode in Delhi\" errors\n",
    "            mask = (np.abs(group['lat'] - lat_mean) <= 2 * lat_std) & \\\n",
    "                   (np.abs(group['lon'] - lon_mean) <= 2 * lon_std)\n",
    "            return group[mask]\n",
    "\n",
    "        print(\"   -> Calculating spatial clusters...\")\n",
    "        # Apply filter per district\n",
    "        df_final = df_clean.groupby(['state', 'district'], group_keys=False).apply(filter_spatial_outliers)\n",
    "        \n",
    "        # --- D. SAVE ---\n",
    "        after_count = len(df_final)\n",
    "        outlier_count = len(df_clean) - after_count\n",
    "        \n",
    "        # Drop the temp lat/lon columns before saving to keep structure clean\n",
    "        df_final = df_final.drop(columns=['lat', 'lon'])\n",
    "        \n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        df_final.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"   ✅ Cleaned.\")\n",
    "        print(f\"   - Original Rows: {before_count:,}\")\n",
    "        print(f\"   - Invalid Pincodes Removed: {invalid_count:,}\")\n",
    "        print(f\"   - Spatial Outliers Removed: {outlier_count:,}\")\n",
    "        print(f\"   - Final Level 4 Rows: {after_count:,}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_level4_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e8e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# If your data is in a file called 'data.json'\n",
    "with open('in.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract all \"name\" values from the properties of each feature\n",
    "names = [feature['properties']['name'] for feature in data['features']]\n",
    "\n",
    "# Print the list of names\n",
    "print(len(names))\n",
    "for name in names:\n",
    "    print(name)\n",
    "print(names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
